# CART回归树

## m_s_e计算均方误差

```python
def m_s_e(groups,class_values):
	mean_square_error=0.0
	for group in groups:
		size=len(group)
		if size==0:	continue
		labels=[row[-1] for row in group]
		proportion=np.array(labels).mean()
		error=sum(power(labels - proportion,2))#改动之后可以写成绝对误差
		mean_square_error+=error
	return mean_square_error
```

## split划分数据集

```python
def split(dataSet,index,value):
	#和ID3处的不同，每次不删列
	#value是作为分割线，而不是等于才取
	left,right=list(),list()
	for row in dataSet:
		if row[index]<value:
			left.append(row)
		else:
			right.append(row)

	return left,right
```



## get-split获取最优特征索引与二分标准

```python
def get_split(dataSet):
	class_values=list(set(row[-1] for row in dataSet))
	b_index,b_value,b_score,b_groups=999,999,999,None
	features=range(0,len(dataSet[0])-1)
	for index in features:
		fea_Space=list(set([row[index] for row in dataSet]))
		##这里将每一个值作为二分标准选取方法,可以采用其他方法		
		for row in fea_Space:
			groups=test_split(dataSet,index,row)
			gini=m_q_e(groups,class_values)
			if gini<b_score:
				b_index,b_value,b_score,b_groups=index,row,gini,groups
	return b_index,b_value,b_group
```

## get_nodes递归获取树节点

```python
def get_nodes(data,max_depth=1,minsize=1,depth=1):
	labelList=[d[-1]  for d in data]
	if (len(set(labelList))==1) or len(data)<minsize or depth>=max_depth:
		return to_terminal(labelList)
	if(sum(power(np.array(labelList).mean()-labelList,2))<0.01):
		return to_terminal(labelList)
	node=get_split(data)
	left,right=node[2][0],node[2][1]
	tree={'index':node[0],'value':node[1],'left':[],'right':[]}
	tree['left']=get_nodes(left,max_depth,minsize,depth+1)
	tree['right']=get_nodes(right,max_depth,minsize,depth+1)
	return tree	
```

## create_tree调用get_nodes建树，因为有depth和minsize两个参数

